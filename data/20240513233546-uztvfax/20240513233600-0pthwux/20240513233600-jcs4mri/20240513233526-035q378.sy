{"ID":"20240513233526-035q378","Spec":"1","Type":"NodeDocument","Properties":{"id":"20240513233526-035q378","title":"Elements of statistical learning","type":"doc","updated":"20240513233526"},"Children":[{"ID":"20240513233527-ml23ho3","Type":"NodeParagraph","Properties":{"id":"20240513233527-ml23ho3","updated":"20240513233527"},"Children":[{"Type":"NodeText","Data":"#stats #books"}]},{"ID":"20240513233528-ktmke2a","Type":"NodeParagraph","Properties":{"id":"20240513233528-ktmke2a","updated":"20240513233528"},"Children":[{"Type":"NodeText","Data":"exemple problème supervisé"}]},{"ID":"20240513233529-l16cpkr","Type":"NodeList","ListData":{"BulletChar":42,"Padding":2,"Marker":"Kg==","Num":-1},"Properties":{"id":"20240513233529-l16cpkr","updated":"20240513233529"},"Children":[{"ID":"20240513233530-n769trb","Type":"NodeListItem","Data":"*","ListData":{"BulletChar":42,"Padding":2,"Marker":"Kg==","Num":-1},"Properties":{"id":"20240513233530-n769trb","updated":"20240513233530"},"Children":[{"ID":"20240513233531-7uh56ng","Type":"NodeParagraph","Properties":{"id":"20240513233531-7uh56ng","updated":"20240513233531"},"Children":[{"Type":"NodeText","Data":"Spam : certains mots sont plus fréquents dans les mails classifiés spam"}]}]},{"ID":"20240513233532-200jsj9","Type":"NodeListItem","Data":"*","ListData":{"Tight":true,"BulletChar":42,"Padding":2,"Marker":"Kg==","Num":-1},"Properties":{"id":"20240513233532-200jsj9","updated":"20240513233532"},"Children":[{"ID":"20240513233533-20phh09","Type":"NodeParagraph","Properties":{"id":"20240513233533-20phh09","updated":"20240513233533"},"Children":[{"Type":"NodeText","Data":"PSA en fonction du poids de la prostate, Gleason etc : scatterplot mais non évident (données 1989)"}]}]},{"ID":"20240513233534-kr6g94c","Type":"NodeListItem","Data":"*","ListData":{"Tight":true,"BulletChar":42,"Padding":2,"Marker":"Kg==","Num":-1},"Properties":{"id":"20240513233534-kr6g94c","updated":"20240513233534"},"Children":[{"ID":"20240513233535-mt873mp","Type":"NodeParagraph","Properties":{"id":"20240513233535-mt873mp","updated":"20240513233535"},"Children":[{"Type":"NodeText","Data":"Reconnaissances chiffres sur image 16x16 bits normalisés"}]}]}]},{"ID":"20240513233536-cijqmin","Type":"NodeParagraph","Properties":{"id":"20240513233536-cijqmin","updated":"20240513233536"},"Children":[{"Type":"NodeText","Data":"DNA microarray : pour une liste de gènes, on regarde la quantité d'ARN qui s'hybride par rapport à un échantillon vs une référence (par fluorescence, l'un est rouge l'autre vert). Avec plusieurs échantillons, on a donc une matrice. Quels gènes sont prédictifs -\u003e aprentissage non supervisé"}]},{"ID":"20240513233537-dt8gohc","Type":"NodeList","ListData":{"Typ":1,"Tight":true,"Start":2,"Delimiter":46,"Padding":3,"Marker":"Mg==","Num":2},"Properties":{"id":"20240513233537-dt8gohc","updated":"20240513233537"},"Children":[{"ID":"20240513233538-n3qob32","Type":"NodeListItem","Data":"2","ListData":{"Typ":1,"Tight":true,"Start":2,"Delimiter":46,"Padding":3,"Marker":"Mg==","Num":2},"Properties":{"id":"20240513233538-n3qob32","updated":"20240513233538"},"Children":[{"ID":"20240513233539-82iwh4c","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20240513233539-82iwh4c","updated":"20240513233539"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Overview of supervised learning"}]}]}]},{"ID":"20240513233540-gql2xhq","Type":"NodeParagraph","Properties":{"id":"20240513233540-gql2xhq","updated":"20240513233540"},"Children":[{"Type":"NodeText","Data":"K-nearest neighbors  : pas d'hypothèse sur la distribution (\"low bias\") mais variabilité importante (\"high variance\")."},{"Type":"NodeHardBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Moindres carrés : suppose que ce soit adapté (\"high bias\") mais stable (\"low variance\")"}]},{"ID":"20240513233541-jlg4gve","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20240513233541-jlg4gve","updated":"20240513233541"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"# ","Properties":{"id":""}},{"Type":"NodeText","Data":"Chap 2"}]},{"ID":"20240513233542-qhv410p","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20240513233542-qhv410p","updated":"20240513233542"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Linear vs nearest neighbours"}]},{"ID":"20240513233543-vxb18cm","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":4,"Marker":"LQ==","Num":-1},"Properties":{"id":"20240513233543-vxb18cm","updated":"20240513233543"},"Children":[{"ID":"20240513233544-fat6exo","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":4,"Marker":"LQ==","Num":-1},"Properties":{"id":"20240513233544-fat6exo","updated":"20240513233544"},"Children":[{"ID":"20240513233545-vaajagd","Type":"NodeParagraph","Properties":{"id":"20240513233545-vaajagd","updated":"20240513233545"},"Children":[{"Type":"NodeText","Data":"linear model fitted by least squared = stable but possibly inaccurate"}]}]},{"ID":"20240513233546-se1dlaj","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":4,"Marker":"LQ==","Num":-1},"Properties":{"id":"20240513233546-se1dlaj","updated":"20240513233546"},"Children":[{"ID":"20240513233547-b2ieubl","Type":"NodeParagraph","Properties":{"id":"20240513233547-b2ieubl","updated":"20240513233547"},"Children":[{"Type":"NodeText","Data":"nearest neighbours = precise but unstable"}]}]}]},{"ID":"20240513233548-v1hdoas","Type":"NodeParagraph","Properties":{"id":"20240513233548-v1hdoas","updated":"20240513233548"},"Children":[{"Type":"NodeText","Data":"If the data is a set of tightly lustered Gaussians (theirs means are distributed as Gaussian), the optimal will be nonlinear and disjoint If the data is a mixture of unrelated Gaussian, linear is almost optimal"}]},{"ID":"20240513233549-cpb2a9h","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20240513233549-cpb2a9h","updated":"20240513233549"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Bias, variance"}]},{"ID":"20240513233550-i520y2z","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20240513233550-i520y2z","updated":"20240513233550"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Bias"}]},{"ID":"20240513233551-9ps4ldp","Type":"NodeMathBlock","Properties":{"id":"20240513233551-9ps4ldp","updated":"20240513233551"},"Children":[{"Type":"NodeMathBlockOpenMarker","Properties":{"id":""}},{"Type":"NodeMathBlockContent","Data":"Bias_\\theta(\\hat{\\theta})) = E_{x|\\theta}(\\hat{\\theta}) - \\theta","Properties":{"id":""}},{"Type":"NodeMathBlockCloseMarker","Properties":{"id":""}}]},{"ID":"20240513233552-lge792k","Type":"NodeParagraph","Properties":{"id":"20240513233552-lge792k","updated":"20240513233552"},"Children":[{"Type":"NodeText","Data":"where $E_{x|\\theta}$ is the expected value over "},{"Type":"NodeBackslash","Properties":{"id":""},"Children":[{"Type":"NodeBackslashContent","Data":"$","Properties":{"id":""}}]},{"Type":"NodeText","Data":"(x"},{"Type":"NodeBackslash","Properties":{"id":""},"Children":[{"Type":"NodeBackslashContent","Data":"|","Properties":{"id":""}}]},{"Type":"NodeText","Data":"θ) (average over all possible observations x with θ fixed)"}]},{"ID":"20240513233553-cwt6cdv","Type":"NodeParagraph","Properties":{"id":"20240513233553-cwt6cdv","updated":"20240513233553"},"Children":[{"Type":"NodeText","Data":"Error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting)."}]},{"ID":"20240513233554-k5ik16w","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20240513233554-k5ik16w","updated":"20240513233554"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Variance :"}]},{"ID":"20240513233555-2k1fvil","Type":"NodeParagraph","Properties":{"id":"20240513233555-2k1fvil","updated":"20240513233555"},"Children":[{"Type":"NodeText","Data":"Measure the dispersion : $$Variance(X) = E(X - E(X))^2$$"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting)."}]},{"ID":"20240513233556-torl65s","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20240513233556-torl65s","updated":"20240513233556"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Curse of dimensionality = sparse sampling in hihg dimensions -"},{"Type":"NodeBackslash","Properties":{"id":""},"Children":[{"Type":"NodeBackslashContent","Data":"\u003e","Properties":{"id":""}}]},{"Type":"NodeText","Data":" it's harder to have enough training data"}]},{"ID":"20240513233557-bsi1uvo","Type":"NodeBlockquote","Properties":{"id":"20240513233557-bsi1uvo","updated":"20240513233557"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20240513233558-83klmwn","Type":"NodeParagraph","Properties":{"id":"20240513233558-83klmwn","updated":"20240513233558"},"Children":[{"Type":"NodeText","Data":"we saw that squared error loss lead us to the regression function f"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"(x) = E(Y "},{"Type":"NodeBackslash","Properties":{"id":""},"Children":[{"Type":"NodeBackslashContent","Data":"|","Properties":{"id":""}}]},{"Type":"NodeText","Data":"X = x) for a quantitative response. The class of"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"nearest-neighbor methods can be viewed as direct estimates of this"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"conditional expectation, but we have seen that they can fail in at"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"least two ways: • if the dimension of the input space is high, the"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"nearest neighbors need not be close to the target point, and can"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"result in large errors; • if special structure is known to exist, this"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"can be used to reduce both the bias and the variance of the estimates."}]}]},{"ID":"20240513233559-d5dkmza","Type":"NodeParagraph","Properties":{"id":"20240513233559-d5dkmza","updated":"20240513233559"},"Children":[{"Type":"NodeText","Data":"Other models are design to overcome the dimensionality problems"}]}]}